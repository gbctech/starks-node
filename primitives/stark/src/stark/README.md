## STARK domains
Proof generation and verification involve (among other things) interpolating and/or evaluating a bunch of polynomials. These polynomials are interpolated and evaluated over various domains, and it is useful to understand what these domains are.

In general, domains for STARKs consist of successive powers of primitive roots of unity. Specifically:

<p align="center">
1, ω, ω<sup>2</sup>, ω<sup>3</sup>, . . . , ω<sup>n-1</sup>
</p>

where *ω* is the *n*-th primitive root of unity. You can also think of *n* as the size of the domain and of *ω* as the domain's generator.

There are 3 domains with which we'll be working:

1. Domain of the trace table or *D<sub>trace</sub>* generated by *ω<sub>trace</sub>*. The size of this domain is equal to the length of the execution trace, and it is the smallest domain out of the three.
2. Constraint evaluation domain or *D<sub>ev</sub>* generated by *ω<sub>ev</sub>*. This domain is bigger than the trace domain by a factor of `MAX_CONSTRAINT_DEGREE`. Currently, `MAX_CONSTRAINT_DEGREE` is 8, so the constraint evaluation domain is 8 times bigger than the trace domain.
3. Low degree extension domain or *D<sub>lde</sub>* generated by *ω<sub>lde</sub>*. This domain is bigger than the trace domain by they `extension_factor` parameter. `extension_factor` must be at least 16 (but may be significantly bigger) - so, LDE domain is the biggest one of the three.

## Proof generation

To generate a STARK proof we use `prove()` function from the [prover](prover.rs) module. The function takes the following parameters:

* **trace** - an execution [trace table](trace) resulting from executing a program. The trace table is instantiated in the [processor](../processor) module, and then passed into the `prove()` function.
* **inputs** - a list of values to which the stack is initialized at the first step of the computation.
* **outputs** - a list of values which must be on the stack at the last step of the computation.
* **options** - [config options](options.rs) for proof generation. These control trade offs between proof size, proving time, and security level.

At the high level, proof generation process consists of the following 9 steps.

### 1. Extend execution trace
We can think of register traces in the execution trace table as evaluations of trace polynomials *T<sub>i</sub>(x)*. Thus, each row in the trace table can be written as:

<p align="center">
T<sub>0</sub>(ω<sup>i</sup><sub>trace</sub>), T<sub>1</sub>(ω<sup>i</sup><sub>trace</sub>), T<sub>2</sub>(ω<sup>i</sup><sub>trace</sub>), . . ., T<sub>k - 1</sub>(ω<sup>i</sup><sub>trace</sub>)
</p>

where, *k* is the number of registers, and *i* is the index of the row (same as the step of the computation).

To extend the trace table, we need to evaluate these polynomials over a larger domain. But since the trace table consists of polynomial evaluations, we first need to interpolate each register trace into a polynomial. We do this by running inverse FFT.

Then, trace polynomials are evaluated over *D<sub>lde</sub>* to generate the extended trace table (this is done by running froward FFT). Each row in the extended trace table can be written as:

<p align="center">
T<sub>0</sub>(ω<sup>i</sup><sub>lde</sub>), T<sub>1</sub>(ω<sup>i</sup><sub>lde</sub>), T<sub>2</sub>(ω<sup>i</sup><sub>lde</sub>), . . . T<sub>k - 1</sub>(ω<sup>i</sup><sub>lde</sub>)
</p>

A couple of things to note:
1. The degree of trace polynomials is one less than trace length, or *deg(T<sub>i</sub>(x)) = |D<sub>trace</sub>| - 1* for all *i*.
2. This is the most computationally intensive part of proof generation. Depending on the `extension_factor` used, it can take up between 40% and 80% of proof generation time (in single-threaded mode).

### 2. Build trace Merkle tree
After the execution trace table has been extended, we build a Merkle tree from the extended register traces. Leaves in the resulting tree will have the following form:

<p align="center">
Leaf<sub>i</sub> = (T<sub>0</sub>(ω<sup>i</sup><sub>lde</sub>), T<sub>1</sub>(ω<sup>i</sup><sub>lde</sub>), T<sub>2</sub>(ω<sup>i</sup><sub>lde</sub>), . . . T<sub>k - 1</sub>(ω<sup>i</sup><sub>lde</sub>))
</p>

### 3. Evaluate constraints
The next step is to evaluate constraints. The actual constraint definitions are described [here](constraints). Our eventual goal is to combine all constraints into a single *constraint polynomial*. We do this by computing a random linear combination of all constraints like so:

<p align="center">
<img src="https://render.githubusercontent.com/render/math?math=\large C(x) = \sum_k (\alpha_{2k} \cdot C_k(x) %2B \alpha_{2k%2B1} \cdot C_k(x) \cdot x^{d_k})">
</p>

where:
* *x = ω<sup>i</sup><sub>ev</sub>* for all *i* in the constraint evaluation domain.
* *C<sub>0</sub> ... C<sub>k-1</sub>* are the individual constraint evaluation functions.
* *α<sub>0</sub> ... α<sub>2k-1</sub>* are the coefficients for the random linear combination. These coefficients are derived using PRNG seeded with the root of the trace Merkle tree we built in the previous step.
* *d<sub>0</sub> ... d<sub>k-1</sub>* are the adjustment degrees needed to guarantee that constraint degrees are enforced exactly. Adjustment degrees are calculated as: *d<sub>k</sub> = [target degree] - deg(C<sub>k</sub>(x))*.

However, in this step, we don't compute the full constraint polynomial. Instead, we compute linear combinations of constraint numerators only. In the next step, we'll divide these linear combinations by their respective denominators. This allows us to minimize the number of divisions (which are expensive) and also reduces the amount of RAM needed to hold all constraint evaluations. Since our constraints can have 3 possible denominators, we'll still need to keep track of 3 separate linear combinations but that's much better than keeping track of 30+ individual constraint evaluations.

The 3 combinations we need to keep track of are:

1. Combination of transition constraints. The denominator for this combination is *(x<sup>n</sup> - 1) / (x - ω<sub>trace</sub><sup>(n-1)</sup>)*.
2. Combination of boundary constraints at the first step. The denominator for this combination is *(x - 1)*.
3. Combination of boundary constraints at the last step. The denominator for this combination is *(x - ω<sub>trace</sub><sup>(n-1)</sup>)*.

Because the denominators above have different degrees, *target degrees* for the linear combinations will be different. Specifically:
* Target degree for transition constraint combination will be *|D<sub>ev</sub>| - 1*.
* Target degree for boundary constraint combination will be *|D<sub>ev</sub>| - |D<sub>trace</sub>| + 1*

This way, when linear combinations are divided by their respective denominator, their degrees will align, and the degree for the final *constraint polynomial* will be:

<p align="center">
deg(C(x)) = |D<sub>ev</sub>| - |D<sub>trace</sub>|
</p>

For example, if our execution trace is 16 steps long:
* Transition constraint combination degree will be `16 * 8 - 1 = 127`.
* Boundary constraint combination degree will be `16 * 8 - 16 + 1 = 113`.
* Once the denominators are divided out, the final degree of the *constraint polynomial* will be `112`.

### 4. Convert constraint evaluations into a single polynomial
After constraints have been evaluated and combined into the 3 linear combinations, we do the following:
1. Interpolate each combination of constraint evaluation into a polynomial,
2. Divide out denominators from their respective polynomials,
3. Add the resulting polynomials together into the *constraint polynomial C(x)*.

### 5. Build Merkle tree from constraint polynomial evaluations
Once the *constraint polynomial* has been constructed, we evaluate it over *D<sub>lde</sub>* and put the resulting values into a Merkle tree. Since our values are 128 bits (we are working in a 128-bit prime field), but Merkle tree leaves are 256 bits, we put 2 consecutive evaluations into a single leaf like so:

<p align="center">
Leaf<sub>i</sub> = (C(x<sub>2i</sub>), C(x<sub>2i+1</sub>))
</p>

where, *x<sub>i</sub> = ω<sup>i</sup><sub>lde</sub>* for all *i* in the low degree extension domain.

### 6. Build DEEP composition polynomial
Next, we use the root of the tree constructed in the previous step to seed a new PRNG. We then use this PRNG to:

1. Draw a random point *z* from the entire field (the "out-of-domain" point),
2. Draw a set of coefficients for the random linear combination of constraint and trace polynomials.

This new random linear combination is called a *DEEP composition polynomial P(x)*. The degree of this polynomial will be one less than the degree of the constraint polynomial, or *deg(P(x)) = |D<sub>ev</sub>| - |D<sub>trace</sub>| - 1*.

DEEP composition polynomial is constructed as follows:

First, we compute *T<sub>k</sub>(z)* and *T<sub>k</sub>(z * ω<sub>trace</sub>)*, and divide these points out of trace polynomials like so:

<p align="center">
<img src="https://render.githubusercontent.com/render/math?math=\large T^'_k(x) = \frac{T_k(x) - T_k(z)}{x - z}">
</p>

<p align="center">
<img src="https://render.githubusercontent.com/render/math?math=\large T^''_k(x) = \frac{T_k(x) - T_k(z \cdot \omega_{trace})}{x - z \cdot \omega_{trace}}">
</p>

Then, we compute a random linear combination of the resulting polynomials as:

<p align="center">
<img src="https://render.githubusercontent.com/render/math?math=\large T(x) = \sum_k (\alpha_{2k} \cdot T^'_k(x) %2B \alpha_{2k%2B1} \cdot T^''_k(x))">
</p>

where *α<sub>0</sub> ... α<sub>2i-1</sub>* are the coefficients for the random linear combination.

Next, we raise the degree of the combined trace polynomials to make sure it matches the degree of the DEEP composition polynomial:

<p align="center">
<img src="https://render.githubusercontent.com/render/math?math=\large T^'(x) = \alpha \cdot T(x) %2B \beta \cdot T(x) \cdot x^d">
</p>

where:
* *α* and *β* are pseudo-random coefficients,
* *d* is the adjustment degree which is equal to *|D<sub>ev</sub>| - 2 * |D<sub>trace</sub>| + 1*.

Then we divide *z* point out of the constraint polynomial like so:

<p align="center">
<img src="https://render.githubusercontent.com/render/math?math=\large C^'(x) = \frac{C(x) - C(z)}{x - z}">
</p>

Finally, we combine this resulting DEEP constraint polynomial with the DEEP trace polynomial like so:

<p align="center">
<img src="https://render.githubusercontent.com/render/math?math=\large P(x) = T^'(x) %2B \gamma \cdot C^'(x)">
</p>

where, *γ* is yet another pseudo-random coefficient.

### 7. Construct FRI layers for the composition polynomial
Next, we evaluate the composition polynomial *P(x)* over *D<sub>lde</sub>*. Remember that *deg(P(x)) = |D<sub>ev</sub>| - |D<sub>trace</sub>| - 1*. So, for example, if our execution trace is 16 steps long, the degree will be: `8 * 16 - 16 - 1 = 111`. If we set `extension_factor` to 64, then *|D<sub>lde</sub>|* = `8 * 64 = 1024`. So, we will have a degree 111 polynomial evaluated over the domain of size 1024. This implies coding rate of 1/8.

Then, we apply radix-4 FRI to compute FRI layers for the composition polynomial evaluations. This means that at every layer we reduce the the domain size and the degree of the polynomial by a factor of 4 until the size of the domain reaches 256.

For the example we used above, FRI layers will look like so:
* Layer 0: domain size 1024, degree 111
* Layer 1: domain size 256, degree 27

The details of FRI proof generation process are described [here](fri).

### 8. Determine query positions
Once Merkle trees for all FRI layers are constructed, we combined roots of these trees into a single value as follows:

<p align="center">
merged_roots = hash(root<sub>0</sub>, root<sub>1</sub>, . . . , root<sub>j - 1</sub>)
</p>

where *j* is the number of FRI layers.

We then perform proof-of-work against this merged root as follows:

1. Take a nonce (at first initialized to 0) and hash it with the merged root.
2. Check if the result satisfies the difficulty threshold specified by the `grinding_factor` config parameter.
3. If the threshold is satisfied, return; otherwise increment the nonce by 1 and repeat.

Once the correct nonce is found, we construct a PRNG seed value as follows:

<p align="center">
seed = hash(merged_root, pow_nonce)
</p>

Then, we instantiate a PRNG with this seed and draw random positions from *D<sub>lde</sub>*. The number of positions drawn is equal to the `num_queries` config parameter.

### 9. Build proof object
Once query positions are determined, we build the [proof object](proof.rs) and return. The proof object consists of the following:

1. Root of the trace Merkle tree we built in step 2.
2. Authentication paths from the root of the trace tree to the queried positions.
3. Values of all trace registers at the queried positions.
4. Root of the constraint Merkle tree we built in step 5.
5. Authentication paths from the root of the constraint tree to the queried positions. The positions against constraint tree need to be adjusted since each leaf in the tree contains 4 consecutive positions of constraint evaluations.
6. Evaluations of trace polynomials at out-of-domain point *z* we computed at step 6. Specifically: *T<sub>k</sub>(z)* and *T<sub>k</sub>(z * ω<sub>trace</sub>)* for all registers *k*.
7. FRI proof which consists of Merkle tree roots and authentication paths to the queried positions at each layer except for the last one. For the last layer, we take the tree root and all evaluations (at most 256 values). Note that query positions at each layer need to be adjusted to account for transpositions that we've done in step 7.
8. Proof-of-work nonce we computed in step 8.

## Proof verification
To verify a STARK proof we use `verify()` function from the [verifier](verifier.rs) module. The function takes the following parameters:

* **program_hash** - hash of the program for which we want to verify correct execution.
* **inputs** - a list of inputs with which the program was executed.
* **outputs** - a list of outputs produced by the program.
* **proof** - a [proof object](proof.rs) generated during program execution on Distaff VM.

At the high level, proof verification process consists of the following 5 steps:

### 1. Verify proof of work and determine query positions
First, we read Merkle tree roots for all FRI layers from the proof, and combine them together as:

<p align="center">
merged_roots = hash(root<sub>0</sub>, root<sub>1</sub>, . . . , root<sub>j - 1</sub>)
</p>

where *j* is the number of FRI layers.

Then, we read proof-of-work nonce from the proof, and use it to build a seed value as follows:

<p align="center">
seed = hash(merged_root, pow_nonce)
</p>

We then verify that the seed value satisfies proof-of-work difficulty target set by the `grinding_factor` config parameter, and use it to instantiate a PRNG.

Finally, we use this PRNG to draw random query positions from *D<sub>lde</sub>*. The number of positions drawn is equal to the `num_queries` config parameter.

### 2. Verify trace and constraint Merkle proofs
Once query positions are determined, we read roots and authentication paths for trace and constraint Merkle trees from the proof.

We then verify the authentication paths against these query positions. This gives us evaluations of trace polynomials *T<sub>k</sub>(x)* and combined constraint polynomial *C(x)* at all queried positions.

### 3. Compute constraint evaluations at DEEP point z
Next, we use constraint Merkle tree root to seed a PRNG and derive the out-of-domain point *z*.

Then, we read *T<sub>k</sub>(z)* and *T<sub>k</sub>(z * ω<sub>trace</sub>)* from the proof, and evaluate constraints against them (see [here](constraints) for more info on constraint evaluation).

Finally, we combine all constraint evaluations at *z* into a single value like so:

<p align="center">
<img src="https://render.githubusercontent.com/render/math?math=\large C(z) = \sum_k (\alpha_{2k} \cdot C_k(z) %2B \alpha_{2k%2B1} \cdot C_k(z) \cdot x^{d_k})">
</p>

where:
* *C<sub>0</sub> ... C<sub>k-1</sub>* are the individual constraint evaluations.
* *α<sub>0</sub> ... α<sub>2k-1</sub>* are the coefficients for the random linear combination. These coefficients are derived using PRNG seeded with the root of the trace Merkle tree.
* *d<sub>0</sub> ... d<sub>k-1</sub>* are the adjustment degrees needed to guarantee that constraint degrees are enforced exactly. These are different for different constraints - see the proof generation section for more info on how adjustment degrees are computed.

The output of this process are constraint evaluations at out-of-domain point: *C<sub>k</sub>(z)*.

### 4. Compute composition polynomial evaluations
At this point, we have:
* Evaluations of *T<sub>k</sub>(x)* at all queried positions;
* Evaluations of *T<sub>k</sub>(x)* at *z* and *z * ω<sub>trace</sub>* ;
* Evaluations of *C(x)* at all queried positions;
* Evaluation of *C(x)* at *z*.

We use this data to compute evaluations of the DEEP composition polynomial *P(x)* at all queried positions. This is done as follows:

First, seed a PRNG with the root of the constraint Merkle tree and use it to derive a set of coefficients for random linear combinations.

Then, divide out DEEP points from their respective evaluations like so:

<p align="center">
<img src="https://render.githubusercontent.com/render/math?math=\large T^'_k(x) = \frac{T_k(x) - T_k(z)}{x - z}">
</p>

<p align="center">
<img src="https://render.githubusercontent.com/render/math?math=\large T^''_k(x) = \frac{T_k(x) - T_k(z \cdot \omega_{trace})}{x - z \cdot \omega_{trace}}">
</p>

<p align="center">
<img src="https://render.githubusercontent.com/render/math?math=\large C^'(x) = \frac{C(x) - C(z)}{x - z}">
</p>

where *x = ω<sup>i</sup><sub>lde</sub>* for all queried positions *i*.

Then, combine resulting trace evaluations at each position like so:

<p align="center">
<img src="https://render.githubusercontent.com/render/math?math=\large T(x) = \sum_k (\alpha_{2k} \cdot T^'_k(x) %2B \alpha_{2k%2B1} \cdot T^''_k(x))">
</p>

where *α<sub>0</sub> ... α<sub>2i-1</sub>* are the coefficients for the random linear combination.

Then, raise the degree of resulting values to match the degree of the composition polynomial like so: 

<p align="center">
<img src="https://render.githubusercontent.com/render/math?math=\large T^'(x) = \alpha \cdot T(x) %2B \beta \cdot T(x) \cdot x^d">
</p>

where:
* *α* and *β* are pseudo-random coefficients,
* *d* is the adjustment degree which is equal to *|D<sub>ev</sub>| - 2 * |D<sub>trace</sub>| + 1*.

Finally, we combine this resulting DEEP constraint evaluations with the DEEP trace evaluations like so:

<p align="center">
<img src="https://render.githubusercontent.com/render/math?math=\large P(x) = T^'(x) %2B \gamma \cdot C^'(x)">
</p>

where, *γ* is yet another pseudo-random coefficient.

The output of this process are the evaluations of the composition polynomial *P(x)* at all queried positions.

### 5. Verify low-degree proof
Once we have *P(x)* evaluations at the queried positions, we read FRI portion of the proof and use it to verify that the degree implied by *P(x)* evaluations is smaller than *|D<sub>ev</sub>| - 2 * |D<sub>trace</sub>|*. FRI verification process is described [here](fri).

## Proof soundness
Proof soundness (in bits) is currently estimated as follows:

<p align="center">
<img src="https://render.githubusercontent.com/render/math?math=\large s = log_2 (\frac{1}{\rho}) \cdot n %2B r">
</p>

where:
* *ρ* is the coding rate which is equal to *[max constraint degree] / [extension factor]*. Max constraint degree is currently fixed at 8 - so, in practical terms, the extension factor determines the coding rate. For example, if extension factor is set to 64, coding rate is 1/8.
* *n* - number of queries.
* *r* - grinding factor.

For example, for the default config values (*ρ = 1/4, n = 50, r = 20*), proof soundness can be estimated as:

<p align="center">
<img src="https://render.githubusercontent.com/render/math?math=\large log_2 4 \cdot 50 %2B 20 = 120">
</p>

In other words, default security level is 120 bits.